import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import numpy as np
from copy import deepcopy

# 设置随机种子保证实验可复现
torch.manual_seed(42)
np.random.seed(42)

# 数据加载和预处理
dataset = pd.read_csv("dataset.csv", sep="\t", header=None)
texts = dataset[0].tolist()
string_labels = dataset[1].tolist()

label_to_index = {label: i for i, label in enumerate(set(string_labels))}
numerical_labels = [label_to_index[label] for label in string_labels]

char_to_index = {'<pad>': 0}
for text in texts:
    for char in text:
        if char not in char_to_index:
            char_to_index[char] = len(char_to_index)

index_to_char = {i: char for char, i in char_to_index.items()}
vocab_size = len(char_to_index)
max_len = 40


# 数据集类（保持不变）
class CharBoWDataset(Dataset):
    def __init__(self, texts, labels, char_to_index, max_len, vocab_size):
        self.texts = texts
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.char_to_index = char_to_index
        self.max_len = max_len
        self.vocab_size = vocab_size
        self.bow_vectors = self._create_bow_vectors()

    def _create_bow_vectors(self):
        tokenized_texts = []
        for text in self.texts:
            tokenized = [self.char_to_index.get(char, 0) for char in text[:self.max_len]]
            tokenized += [0] * (self.max_len - len(tokenized))
            tokenized_texts.append(tokenized)

        bow_vectors = []
        for text_indices in tokenized_texts:
            bow_vector = torch.zeros(self.vocab_size)
            for index in text_indices:
                if index != 0:
                    bow_vector[index] += 1
            bow_vectors.append(bow_vector)
        return torch.stack(bow_vectors)

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.bow_vectors[idx], self.labels[idx]


# 可灵活调整层数和节点数的通用分类器
class FlexibleClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        """
        Args:
            input_dim: 输入维度（词汇表大小）
            hidden_dims: 隐藏层维度列表，例如 [128] 表示1层128节点，[256, 128] 表示2层，分别256和128节点
            output_dim: 输出维度（类别数）
        """
        super(FlexibleClassifier, self).__init__()

        # 构建隐藏层
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            prev_dim = hidden_dim

        # 输出层
        layers.append(nn.Linear(prev_dim, output_dim))

        # 将所有层组合成Sequential
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)


# 训练函数（返回每轮的loss记录）
def train_model(model, dataloader, criterion, optimizer, num_epochs=10):
    """
    训练模型并返回每轮的loss

    Returns:
        epoch_losses: 每轮的平均loss列表
    """
    epoch_losses = []
    model.train()

    for epoch in range(num_epochs):
        running_loss = 0.0
        for idx, (inputs, labels) in enumerate(dataloader):
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # 计算本轮平均loss
        avg_loss = running_loss / len(dataloader)
        epoch_losses.append(avg_loss)
        print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}")

    return epoch_losses


# 准备数据集
char_dataset = CharBoWDataset(texts, numerical_labels, char_to_index, max_len, vocab_size)
dataloader = DataLoader(char_dataset, batch_size=32, shuffle=True)

# 定义要对比的参数组合
param_combinations = [
    {"name": "1层_64节点", "hidden_dims": [64]},
    {"name": "1层_128节点", "hidden_dims": [128]},
    {"name": "1层_256节点", "hidden_dims": [256]},
    {"name": "2层_128-64节点", "hidden_dims": [128, 64]},
    {"name": "2层_256-128节点", "hidden_dims": [256, 128]},
    {"name": "3层_256-128-64节点", "hidden_dims": [256, 128, 64]}
]

# 存储所有参数组合的loss记录
all_loss_records = {}

# 训练不同参数组合的模型
output_dim = len(label_to_index)
num_epochs = 10

for params in param_combinations:
    print("\n" + "=" * 50)
    print(f"训练模型: {params['name']}")
    print("=" * 50)

    # 创建模型
    model = FlexibleClassifier(vocab_size, params["hidden_dims"], output_dim)

    # 定义损失函数和优化器（保持一致）
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # 训练并记录loss
    loss_records = train_model(model, dataloader, criterion, optimizer, num_epochs)
    all_loss_records[params["name"]] = loss_records

# 可视化对比不同模型的loss变化
plt.figure(figsize=(12, 8))
for model_name, losses in all_loss_records.items():
    plt.plot(range(1, num_epochs + 1), losses, marker='o', label=model_name)

plt.xlabel("Epoch")
plt.ylabel("Average Loss")
plt.title("不同层数/节点数的模型Loss对比")
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(range(1, num_epochs + 1))
plt.savefig("model_loss_comparison.png")
plt.show()

# 打印最终loss对比表
print("\n" + "=" * 60)
print("各模型最终Loss对比")
print("=" * 60)
for model_name, losses in all_loss_records.items():
    final_loss = losses[-1]
    print(f"{model_name:15s} | 最终Loss: {final_loss:.4f}")

