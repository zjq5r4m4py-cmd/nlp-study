# 正则表达式、TFIDF、BERT、LLM 优缺点对比



|对比维度|正则表达式（Regex）|TFIDF|BERT|LLM（大语言模型，如GPT、Llama等）|
|---|---|---|---|---|
|**核心优势**|1. 速度极快，执行效率高，无需训练，即时生效；2. 逻辑透明，可精准控制匹配规则，结果可解释性极强；3. 轻量无依赖，无需算力支持，适配简单场景；4. 对结构化文本、固定格式匹配（如手机号、邮箱）效果极佳。|1. 实现简单，计算成本低，易于理解和部署；2. 能快速捕捉文本关键词权重，适配文本检索、分类初筛场景；3. 对小样本、短文本的处理效率较高，无复杂训练流程；4. 可量化文本相似度，输出可解释（权重可追溯）。|1. 具备上下文语义理解能力，能解决一词多义、歧义问题；2. 捕捉文本深层语义特征，适配语义匹配、情感分析等复杂NLP任务；3. 预训练模型可微调，适配特定领域（如医疗、法律），泛化性优于传统方法；4. 支持中文等多语言，对文本语序、语境敏感。|1. 语义理解能力极强，可处理复杂语境、模糊需求（如开放式问答、创意生成）；2. 支持多任务通用，无需针对单一任务微调（零样本/少样本能力突出）；3. 可生成连贯、自然的文本，适配创作、总结、对话等生成式场景；4. 能理解复杂指令，支持跨模态（文本+图片等）、逻辑推理。|
|**核心缺点**|1. 无语义理解能力，仅匹配字符形式，无法处理歧义、同义词；2. 规则维护成本高，场景变化（如匹配规则调整）需重新编写正则；3. 对非结构化文本、模糊匹配场景适配极差；4. 无法泛化，仅能处理预设规则内的情况，鲁棒性弱。|1. 无上下文语义理解，仅统计词频，忽略语序、语境和词的语义关联；2. 对低频关键词、生僻词不友好，易忽略重要语义信息；3. 无法处理一词多义，同义词视为不同词汇，泛化性差；4. 不适配复杂语义任务（如情感分析、语义推理）。|1. 算力成本高，部署和微调需要一定的GPU资源；2. 模型体积较大，轻量部署难度高，不适配边缘设备；3. 对长文本处理能力有限（输入长度有上限）；4. 生成能力弱，仅擅长理解和特征提取，无法完成连贯文本生成。|1. 算力和部署成本极高，模型庞大，推理速度较慢（相较于前三者）；2. 可解释性极差，无法追溯决策逻辑（“黑盒模型”）；3. 易产生幻觉（生成虚假信息），对事实性问题的准确性需校验；4. 训练和微调难度大，需要大量标注数据和专业技术；5. 存在隐私风险，处理敏感数据时安全性需额外保障。|

